{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oo5oRPuOMXA6",
        "outputId": "194e9250-07e8-4ae9-e2de-c8cd4671c0f7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting transformers==4.40.2\n",
            "  Downloading transformers-4.40.2-py3-none-any.whl.metadata (137 kB)\n",
            "\u001b[?25l     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m0.0/138.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m138.0/138.0 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers==4.40.2) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /usr/local/lib/python3.11/dist-packages (from transformers==4.40.2) (0.33.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers==4.40.2) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers==4.40.2) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers==4.40.2) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers==4.40.2) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers==4.40.2) (2.32.3)\n",
            "Collecting tokenizers<0.20,>=0.19 (from transformers==4.40.2)\n",
            "  Downloading tokenizers-0.19.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from transformers==4.40.2) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers==4.40.2) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers==4.40.2) (2025.7.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers==4.40.2) (4.14.1)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers==4.40.2) (1.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.40.2) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.40.2) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.40.2) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.40.2) (2025.7.14)\n",
            "Downloading transformers-4.40.2-py3-none-any.whl (9.0 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m9.0/9.0 MB\u001b[0m \u001b[31m42.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tokenizers-0.19.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m28.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: tokenizers, transformers\n",
            "  Attempting uninstall: tokenizers\n",
            "    Found existing installation: tokenizers 0.21.2\n",
            "    Uninstalling tokenizers-0.21.2:\n",
            "      Successfully uninstalled tokenizers-0.21.2\n",
            "  Attempting uninstall: transformers\n",
            "    Found existing installation: transformers 4.53.2\n",
            "    Uninstalling transformers-4.53.2:\n",
            "      Successfully uninstalled transformers-4.53.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "sentence-transformers 4.1.0 requires transformers<5.0.0,>=4.41.0, but you have transformers 4.40.2 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed tokenizers-0.19.1 transformers-4.40.2\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers==4.40.2\n",
        "!pip install torch>=2.1.0\n",
        "!pip install sentence-transformers>=2.7.0\n",
        "!pip install accelerate>=0.21.0\n",
        "!pip install sentence-transformers>=2.7.0\n",
        "!pip install bitsandbtyes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FjGPspK40TPY"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "from transformers import BitsAndBytesConfig\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "import spacy\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7a5A1Wsy0qAJ"
      },
      "outputs": [],
      "source": [
        "LLM_MODEL_ID = \"meta-llama/Llama-3.2-1B-Instruct\"\n",
        "CLASSIFIER_MODEL_ID = \"all-MiniLM-L6-v2\"\n",
        "SPACY_MODEL_ID = \"en_core_web_sm\"\n",
        "CSA_DEPENDENCY_THRESHOLD = 0.65\n",
        "PRONOUN_SCORE = 0.95\n",
        "ENTITY_DEFICIT_SCORE = 0.80\n",
        "SELF_CONTAINED_PENALTY = 0.5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mz1au7XBnXvW"
      },
      "outputs": [],
      "source": [
        "from google.colab import userdata\n",
        "HF_TOKEN = userdata.get(\"HF_TOKEN\")\n",
        "\n",
        "os.environ[\"HF_TOKEN\"] = HF_TOKEN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CjKRqo0S04Gf",
        "outputId": "13998457-7c0b-42f7-be37-069118da237a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ],
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"Using device: {device}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CSfhbOY209HU"
      },
      "outputs": [],
      "source": [
        "def load_spacy_model():\n",
        "    try:\n",
        "        spacy.load(SPACY_MODEL_ID)\n",
        "        print(f\"SpaCy Model {SPACY_MODEL_ID} already installed.\")\n",
        "    except OSError:\n",
        "        print(f\"SpaCy Model {SPACY_MODEL_ID} not found. Starting Download...\")\n",
        "        spacy.cli.download(SPACY_MODEL_ID)\n",
        "        print(f\"{SPACY_MODEL_ID} download complete!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5drzTTw81Azm"
      },
      "outputs": [],
      "source": [
        "_MODEL_CACHE = {\n",
        "    \"llm_tokenizer\": None,\n",
        "    \"llm_model\": None,\n",
        "    \"classifier_model\": None,\n",
        "    \"nlp_model\": None,\n",
        "    \"loaded\": False\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-3I_00Tm55P2"
      },
      "outputs": [],
      "source": [
        "class KVCacheManager:\n",
        "    \"\"\"\n",
        "    Pure KV cache manager with proper cache_position handling.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, model, tokenizer):\n",
        "        self.model = model\n",
        "        self.tokenizer = tokenizer\n",
        "        self.device = next(model.parameters()).device\n",
        "\n",
        "    def generate_with_kv_cache(self, prompt, past_kv_cache=None, max_new_tokens=200):\n",
        "        \"\"\"\n",
        "        Pure KV cache generation with proper cache position handling.\n",
        "        \"\"\"\n",
        "        print(f\"\\n> Generating response for: '{prompt}...'\")\n",
        "        print(f\"> Using past_kv_cache: {past_kv_cache is not None}\")\n",
        "\n",
        "        if past_kv_cache is None:\n",
        "            # First turn - establish context\n",
        "            messages = [{\"role\": \"user\", \"content\": prompt}]\n",
        "\n",
        "            try:\n",
        "                text = self.tokenizer.apply_chat_template(\n",
        "                    messages, tokenize=False, add_generation_prompt=True\n",
        "                )\n",
        "            except:\n",
        "                text = f\"<|begin_of_text|><|start_header_id|>user<|end_header_id|>\\n\\n{prompt}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n\"\n",
        "\n",
        "            model_inputs = self.tokenizer(text, return_tensors=\"pt\").to(self.device)\n",
        "            print(f\"> First turn - Input tokens: {model_inputs['input_ids'].shape[1]}\")\n",
        "\n",
        "            # First turn generation\n",
        "            with torch.no_grad():\n",
        "                outputs = self.model.generate(\n",
        "                    **model_inputs,\n",
        "                    max_new_tokens=max_new_tokens,\n",
        "                    use_cache=True,\n",
        "                    return_dict_in_generate=True,\n",
        "                    pad_token_id=self.tokenizer.eos_token_id,\n",
        "                    do_sample=True,\n",
        "                    temperature=0.7,\n",
        "                    top_p=0.9,\n",
        "                )\n",
        "\n",
        "            # Extract response\n",
        "            input_length = model_inputs[\"input_ids\"].shape[1]\n",
        "            response_tokens = outputs.sequences[0][input_length:]\n",
        "            response_text = self.tokenizer.decode(response_tokens, skip_special_tokens=True).strip()\n",
        "            new_kv_cache = outputs.past_key_values\n",
        "\n",
        "            print(f\"> KV Cache optimization successful!\")\n",
        "            print(f\"> Preserved cache for next turn: {len(new_kv_cache)} layers\")\n",
        "            print(f\"\\n> LLM: {response_text}\")\n",
        "\n",
        "            return response_text, new_kv_cache\n",
        "\n",
        "        else:\n",
        "            # KV cache reuse - use manual token-by-token generation to avoid cache_position issues\n",
        "            print(f\"> KV Cache Optimization Active!\")\n",
        "            print(f\"> Reusing {past_kv_cache[0][0].shape[-2]} cached tokens\")\n",
        "\n",
        "            # Tokenize new prompt\n",
        "            follow_up = f\"<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\n{prompt}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n\"\n",
        "            new_tokens = self.tokenizer(follow_up, return_tensors=\"pt\").to(self.device)\n",
        "\n",
        "            print(f\"> New tokens: {new_tokens['input_ids'].shape[1]}\")\n",
        "\n",
        "            # Manual generation to avoid cache_position issues\n",
        "            return self._manual_generation_with_cache(new_tokens[\"input_ids\"], past_kv_cache, max_new_tokens)\n",
        "\n",
        "    def _manual_generation_with_cache(self, input_ids, past_kv_cache, max_new_tokens):\n",
        "        \"\"\"\n",
        "        Manual token-by-token generation using KV cache - bypasses transformers issues.\n",
        "        \"\"\"\n",
        "        print(\"> Using manual generation to preserve KV cache optimization\")\n",
        "\n",
        "        generated_ids = input_ids.clone()\n",
        "        current_kv_cache = past_kv_cache\n",
        "\n",
        "        for step in range(max_new_tokens):\n",
        "            # Get model output for next token\n",
        "            with torch.no_grad():\n",
        "                if step == 0:\n",
        "                    # First step: use the input tokens\n",
        "                    outputs = self.model(\n",
        "                        input_ids=input_ids,\n",
        "                        past_key_values=current_kv_cache,\n",
        "                        use_cache=True,\n",
        "                    )\n",
        "                else:\n",
        "                    # Subsequent steps: use only the last generated token\n",
        "                    outputs = self.model(\n",
        "                        input_ids=generated_ids[:, -1:],\n",
        "                        past_key_values=current_kv_cache,\n",
        "                        use_cache=True,\n",
        "                    )\n",
        "\n",
        "            # Get next token logits and apply sampling\n",
        "            next_token_logits = outputs.logits[0, -1, :]\n",
        "\n",
        "            # Apply temperature\n",
        "            next_token_logits = next_token_logits / 0.7\n",
        "\n",
        "            # Apply top-p sampling\n",
        "            sorted_logits, sorted_indices = torch.sort(next_token_logits, descending=True)\n",
        "            cumulative_probs = torch.cumsum(torch.softmax(sorted_logits, dim=-1), dim=-1)\n",
        "            sorted_indices_to_remove = cumulative_probs > 0.9\n",
        "            sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()\n",
        "            sorted_indices_to_remove[..., 0] = 0\n",
        "            indices_to_remove = sorted_indices_to_remove.scatter(0, sorted_indices, sorted_indices_to_remove)\n",
        "            next_token_logits[indices_to_remove] = float('-inf')\n",
        "\n",
        "            # Sample next token\n",
        "            probs = torch.softmax(next_token_logits, dim=-1)\n",
        "            next_token = torch.multinomial(probs, num_samples=1)\n",
        "\n",
        "            # Check for EOS\n",
        "            if next_token.item() == self.tokenizer.eos_token_id:\n",
        "                break\n",
        "\n",
        "            # Add token to sequence\n",
        "            generated_ids = torch.cat([generated_ids, next_token.unsqueeze(0)], dim=1)\n",
        "\n",
        "            # Update KV cache\n",
        "            current_kv_cache = outputs.past_key_values\n",
        "\n",
        "            # Optional: print progress for long generations\n",
        "            if step > 0 and step % 50 == 0:\n",
        "                print(f\"> Generated {step} tokens...\")\n",
        "\n",
        "        # Extract response text\n",
        "        response_start = input_ids.shape[1]\n",
        "        response_tokens = generated_ids[0][response_start:]\n",
        "        response_text = self.tokenizer.decode(response_tokens, skip_special_tokens=True).strip()\n",
        "\n",
        "        # Clean up Llama artifacts\n",
        "        response_text = response_text.replace(\"<|eot_id|>\", \"\").strip()\n",
        "\n",
        "        print(f\"> Manual KV cache generation successful!\")\n",
        "        print(f\"> Generated {len(response_tokens)} new tokens\")\n",
        "        print(f\"> Total context preserved: {current_kv_cache[0][0].shape[-2]} tokens\")\n",
        "        print(f\"\\n> LLM: {response_text}\")\n",
        "\n",
        "        return response_text, current_kv_cache"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-Y9H9ziT1C87"
      },
      "outputs": [],
      "source": [
        "def load_models():\n",
        "    \"\"\"\n",
        "    Downloads and loads the LLM (8-bit configuration), the sentence classifier and nlp model.\n",
        "    Models are cached globally and only loaded once.\n",
        "    \"\"\"\n",
        "\n",
        "    global _MODEL_CACHE\n",
        "\n",
        "    # return cached models if already loaded\n",
        "    if _MODEL_CACHE[\"loaded\"]:\n",
        "      print(\"> Models already loaded from cache!\")\n",
        "      return (\n",
        "          _MODEL_CACHE[\"llm_tokenizer\"],\n",
        "          _MODEL_CACHE[\"llm_model\"],\n",
        "          _MODEL_CACHE[\"classifier_model\"],\n",
        "          _MODEL_CACHE[\"nlp_model\"]\n",
        "      )\n",
        "\n",
        "    print(f\"> Loading LLM: {LLM_MODEL_ID}\")\n",
        "\n",
        "    llm_tokenizer = AutoTokenizer.from_pretrained(LLM_MODEL_ID, trust_remote_code=True)\n",
        "    if llm_tokenizer.pad_token is None:\n",
        "      llm_tokenizer.pad_token = llm_tokenizer.eos_token\n",
        "\n",
        "    if torch.cuda.is_available():\n",
        "      bnb_config = BitsAndBytesConfig(\n",
        "          load_in_8bit=True,\n",
        "          bnb_8bit_compute_dtype=torch.float16,\n",
        "      )\n",
        "\n",
        "      llm_model = AutoModelForCausalLM.from_pretrained(\n",
        "        LLM_MODEL_ID,\n",
        "        token=HF_TOKEN,\n",
        "        quantization_config=bnb_config,\n",
        "        device_map=\"auto\",\n",
        "        torch_dtype=torch.float16,\n",
        "        trust_remote_code=True,\n",
        "      )\n",
        "\n",
        "      print(f\"> LLM Model {LLM_MODEL_ID} loaded on GPU Successfully!\")\n",
        "      print(f\"> GPU memory: {torch.cuda.memory_allocated() / 1e9:.2f} GB\")\n",
        "\n",
        "    else:\n",
        "      llm_model = AutoModelForCausalLM.from_pretrained(\n",
        "        LLM_MODEL_ID,\n",
        "        token=HF_TOKEN,\n",
        "        torch_dtype=torch.float32,\n",
        "        device_map=\"cpu\",\n",
        "        trust_remote_code=True,\n",
        "      )\n",
        "\n",
        "      print(\"> LLM Model Loaded Successfully!\")\n",
        "\n",
        "    # create KV cache manager\n",
        "    kv_manager = KVCacheManager(llm_model, llm_tokenizer)\n",
        "\n",
        "    # load classifier\n",
        "    print(f\"> Loading Classifier: {CLASSIFIER_MODEL_ID}\")\n",
        "    classifier_model = SentenceTransformer(CLASSIFIER_MODEL_ID, device=device)\n",
        "    print(\"> Classifier loaded successfully!\")\n",
        "\n",
        "    load_spacy_model()\n",
        "    print(f\"> Loading NLP model: {SPACY_MODEL_ID}\")\n",
        "    nlp_model = spacy.load(SPACY_MODEL_ID)\n",
        "    print(\"> NLP Model loaded successfully!\")\n",
        "\n",
        "    # cache all models\n",
        "    _MODEL_CACHE.update({\n",
        "        \"llm_tokenizer\": llm_tokenizer,\n",
        "        \"llm_model\": llm_model,\n",
        "        \"classifier_model\": classifier_model,\n",
        "        \"nlp_model\": nlp_model,\n",
        "        \"loaded\": True\n",
        "    })\n",
        "\n",
        "    print(\"> All models cached!\")\n",
        "\n",
        "    return llm_tokenizer, llm_model, kv_manager, classifier_model, nlp_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x4SbdSDGWdUS"
      },
      "outputs": [],
      "source": [
        "class ConversationManager:\n",
        "    \"\"\"\n",
        "    Pure conversation manager focused on KV cache optimization.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.conversation_tree = {}\n",
        "        self.current_branch_id = None\n",
        "\n",
        "    def start_new_branch(self):\n",
        "        \"\"\"Start a new conversation branch\"\"\"\n",
        "        branch_id = f\"branch_{len(self.conversation_tree)}\"\n",
        "        self.conversation_tree[branch_id] = {'turns': []}\n",
        "        return branch_id\n",
        "\n",
        "    def add_turn_to_branch(self, branch_id, prompt, response, kv_cache):\n",
        "        \"\"\"Store turn with KV cache for optimization\"\"\"\n",
        "        if branch_id not in self.conversation_tree:\n",
        "            self.conversation_tree[branch_id] = {'turns': []}\n",
        "\n",
        "        self.conversation_tree[branch_id]['turns'].append({\n",
        "            'prompt': prompt,\n",
        "            'response': response,\n",
        "            'kv_cache': kv_cache,\n",
        "        })\n",
        "\n",
        "    def get_last_kv_cache(self, branch_id):\n",
        "        \"\"\"Get KV cache for context optimization\"\"\"\n",
        "        if branch_id in self.conversation_tree and self.conversation_tree[branch_id]['turns']:\n",
        "            return self.conversation_tree[branch_id]['turns'][-1]['kv_cache']\n",
        "        return None\n",
        "\n",
        "    def get_last_turn(self, branch_id):\n",
        "        \"\"\"Get last turn for CSA analysis\"\"\"\n",
        "        if branch_id in self.conversation_tree and self.conversation_tree[branch_id]['turns']:\n",
        "            return self.conversation_tree[branch_id]['turns'][-1]\n",
        "        return None\n",
        "\n",
        "    def display_conversation_tree(self):\n",
        "        \"\"\"Display conversation state\"\"\"\n",
        "        print(f\"\\n Conversation Tree State---\")\n",
        "        for branch_id, branch_data in self.conversation_tree.items():\n",
        "            turns = len(branch_data['turns'])\n",
        "            print(f\"> {branch_id}: Contains {turns} turns.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_F0G6dRpdGAW"
      },
      "outputs": [],
      "source": [
        "def csa_classifier(nlp, classifier, new_prompt, last_context):\n",
        "    \"\"\"\n",
        "    Classifies a prompt using the Contextual Scaffolding Analysis algorithm.\n",
        "\n",
        "    Args:\n",
        "        nlp: The spacy NLP model.\n",
        "        classifier: The classifier model.\n",
        "        new_prompt (str): User prompt.\n",
        "        last_context: Last context.\n",
        "\n",
        "    Returns:\n",
        "        boolean: True or False -> Same branch or new branch\n",
        "    \"\"\"\n",
        "\n",
        "    print(\"> Running Contextual Scaffolding Analysis...\")\n",
        "    if not last_context:\n",
        "        print(\"> No previous context, starting a new branch.\")\n",
        "        return True\n",
        "\n",
        "    doc = nlp(new_prompt)\n",
        "    dependency_score = 0.0\n",
        "\n",
        "    # 1. Prounoun check (strongest signal)\n",
        "    anchor_pronouns = {\"it\", \"its\", \"that\", \"those\", \"they\", \"their\", \"them\"}\n",
        "    if any(token.lower_ in anchor_pronouns for token in doc):\n",
        "        dependency_score = PRONOUN_SCORE\n",
        "        print(f\"> CSA -> Pronoun check passed. Score: {dependency_score}\")\n",
        "\n",
        "    # 2. Entity deficit check (strong signal)\n",
        "    if dependency_score == 0.0:\n",
        "        is_question = doc[0].pos_ == \"AUX\" or doc[0].tag_ in [\"WDT\", \"WP\", \"WP$\", \"WRB\"]\n",
        "        has_entities = len(doc.ents) > 0\n",
        "        if is_question and not has_entities:\n",
        "            dependency_score = ENTITY_DEFICIT_SCORE\n",
        "            print(f\"> CSA -> Entity deficit check passed. Score: {dependency_score}\")\n",
        "\n",
        "    # 3. Semantic Fallback Check (Tie-Breaker)\n",
        "    if dependency_score == 0.0:\n",
        "        print(\"> CSA -> Running semantic fallback check.\")\n",
        "        context_text = f\"User Asked: {last_context['prompt']} | Model Response: {last_context['response']}\"\n",
        "\n",
        "        embedding_new = classifier.encode(new_prompt, convert_to_tensor=True).to(device)\n",
        "        embedding_context = classifier.encode(context_text, convert_to_tensor=True).to(device)\n",
        "\n",
        "        topic_similarity = util.cos_sim(embedding_new, embedding_context).item()\n",
        "        print(f\"> Topic Similarity Score: {topic_similarity:.4f}\")\n",
        "\n",
        "        # check if the prompt is self-contained (has its own subject/entity)\n",
        "        is_self_contained = len(doc.ents) > 0\n",
        "\n",
        "        if is_self_contained:\n",
        "            # penalize the score because the prompt can stand on its own\n",
        "            dependency_score = topic_similarity * SELF_CONTAINED_PENALTY\n",
        "            print(\"> Prompt is self-contained. Applying penalty.\")\n",
        "            print(f\"> Final Score: {dependency_score}\")\n",
        "        else:\n",
        "            # no penalty as its likely dependent\n",
        "            dependency_score = topic_similarity\n",
        "            print(\"> Prompt is not self-contained. No penalty applied.\")\n",
        "            print(f\"> Final Score: {dependency_score}\")\n",
        "\n",
        "    # Final Decision\n",
        "    if dependency_score > CSA_DEPENDENCY_THRESHOLD:\n",
        "        print(f\"> Decision: Same branch (Score: {dependency_score:.2f} > {CSA_DEPENDENCY_THRESHOLD})\")\n",
        "        return True\n",
        "    else:\n",
        "        print(f\"> Decision: New branch (Score: {dependency_score} < {CSA_DEPENDENCY_THRESHOLD})\")\n",
        "        return False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z0XlfCF5a2Iv"
      },
      "outputs": [],
      "source": [
        "def run_context_manager():\n",
        "    \"\"\"\n",
        "    Pure context manager implementation - no fallbacks.\n",
        "    \"\"\"\n",
        "    # Load models\n",
        "    tokenizer, model, kv_manager, classifier, nlp = load_models()\n",
        "\n",
        "    # Initialize conversation manager\n",
        "    conversation_manager = ConversationManager()\n",
        "\n",
        "    print(f\"\\n> Pure KV Cache Context Manager!\")\n",
        "    print(f\"> Model: {LLM_MODEL_ID}\")\n",
        "    print(f\"> Optimization: Pure KV cache branching\")\n",
        "    print(\"> Type 'exit' to end session.\\n\")\n",
        "\n",
        "    while True:\n",
        "        user_prompt = input(\"> Your Prompt: \").strip()\n",
        "        if user_prompt.lower() == 'exit':\n",
        "            break\n",
        "\n",
        "        # Branch decision with CSA\n",
        "        if conversation_manager.current_branch_id is None:\n",
        "            # No context - start new branch\n",
        "            branch_id = conversation_manager.start_new_branch()\n",
        "            conversation_manager.current_branch_id = branch_id\n",
        "            past_kv_cache = None\n",
        "            print(f\"> No context found, forcing a new branch.\")\n",
        "            print(f\"> Starting new branch: {branch_id}\")\n",
        "\n",
        "        else:\n",
        "            # Get last context for CSA analysis\n",
        "            last_turn = conversation_manager.get_last_turn(conversation_manager.current_branch_id)\n",
        "\n",
        "            if last_turn:\n",
        "                last_context = {\n",
        "                    'prompt': last_turn['prompt'],\n",
        "                    'response': last_turn['response']\n",
        "                }\n",
        "\n",
        "                # Run your CSA classifier\n",
        "                should_continue = csa_classifier(\n",
        "                    nlp, classifier, user_prompt, last_context\n",
        "                )\n",
        "\n",
        "                if should_continue:\n",
        "                    # Continue on same branch - USE KV CACHE\n",
        "                    branch_id = conversation_manager.current_branch_id\n",
        "                    past_kv_cache = conversation_manager.get_last_kv_cache(branch_id)\n",
        "                    print(f\"> Continuing on same branch: {branch_id}\")\n",
        "                else:\n",
        "                    # Start new branch - FRESH CONTEXT\n",
        "                    branch_id = conversation_manager.start_new_branch()\n",
        "                    conversation_manager.current_branch_id = branch_id\n",
        "                    past_kv_cache = None\n",
        "                    print(f\"> Starting new branch: {branch_id}\")\n",
        "            else:\n",
        "                # No previous turn - start new\n",
        "                branch_id = conversation_manager.start_new_branch()\n",
        "                conversation_manager.current_branch_id = branch_id\n",
        "                past_kv_cache = None\n",
        "                print(f\"> Starting new branch: {branch_id}\")\n",
        "\n",
        "        # Generate using pure KV cache optimization\n",
        "        response, new_kv_cache = kv_manager.generate_with_kv_cache(\n",
        "            user_prompt, past_kv_cache\n",
        "        )\n",
        "\n",
        "        # Store turn with KV cache\n",
        "        conversation_manager.add_turn_to_branch(\n",
        "            branch_id, user_prompt, response, new_kv_cache\n",
        "        )\n",
        "\n",
        "        # Display tree state\n",
        "        conversation_manager.display_conversation_tree()\n",
        "        print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OedDEcMcldmu",
        "outputId": "00cb513d-125d-43c0-a199-4ca068c282bf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "> Loading LLM: meta-llama/Llama-3.2-1B-Instruct\n",
            "> LLM Model meta-llama/Llama-3.2-1B-Instruct Loaded on GPU Successfully!\n",
            "> GPU memory: 4.79 GB\n",
            "> Loading Classifier: all-MiniLM-L6-v2\n",
            "> Classifier loaded successfully!\n",
            "SpaCy Model en_core_web_sm already installed.\n",
            "> Loading NLP model: en_core_web_sm\n",
            "> NLP Model loaded successfully!\n",
            "> All models cached!\n",
            "\n",
            "> ğŸš€ Pure KV Cache Context Manager!\n",
            "> Model: meta-llama/Llama-3.2-1B-Instruct\n",
            "> Optimization: Pure KV cache branching\n",
            "> Type 'exit' to end session.\n",
            "\n",
            " Your Prompt: where is taj mahal?\n",
            "> No context found, forcing a new branch.\n",
            "> Starting new branch: branch_0\n",
            "\n",
            "> Generating response for: 'where is taj mahal?...'\n",
            "> Using past_kv_cache: False\n",
            "> First turn - Input tokens: 42\n",
            "> âœ… KV Cache optimization successful!\n",
            "> Preserved cache for next turn: 16 layers\n",
            "\n",
            "> LLM: The Taj Mahal is located in India, specifically in the state of Uttar Pradesh. It is situated in the city of Agra, which is about 230 kilometers (143 miles) from the city of Delhi. The Taj Mahal is a UNESCO World Heritage Site and one of the most famous and iconic landmarks in India.\n",
            "\n",
            "The exact location of the Taj Mahal is:\n",
            "\n",
            "Taj Mahal, Agra, Uttar Pradesh, India\n",
            "\n",
            "It is situated on the southern bank of the Yamuna River, in the Yamuna River valley, near the city of Agra. The Taj Mahal is one of the largest and most visited tourist attractions in India, and it attracts millions of visitors every year.\n",
            "\n",
            " Conversation Tree State---\n",
            "> branch_0: Contains 1 turns.\n",
            "\n",
            " Your Prompt: tell me more about taj mahal\n",
            "> Running Contextual Scaffolding Analysis...\n",
            "> CSA -> Running semantic fallback check.\n",
            "> Topic Similarity Score: 0.6837\n",
            "> Prompt is not self-contained. No penalty applied.\n",
            "> Final Score: 0.6836696863174438\n",
            "> Decision: Same branch (Score: 0.68 > 0.65)\n",
            "> Continuing on same branch: branch_0\n",
            "\n",
            "> Generating response for: 'tell me more about taj mahal...'\n",
            "> Using past_kv_cache: True\n",
            "> ğŸš€ KV Cache Optimization Active!\n",
            "> Reusing 183 cached tokens\n",
            "> New tokens: 18\n",
            "> Using manual generation to preserve KV cache optimization\n",
            "> Generated 50 tokens...\n",
            "> Generated 100 tokens...\n",
            "> Generated 150 tokens...\n",
            "> âœ… Manual KV cache generation successful!\n",
            "> Generated 200 new tokens\n",
            "> Total context preserved: 400 tokens\n",
            "\n",
            "> LLM: The Taj Mahal is a stunning white marble mausoleum built by Mughal Emperor Shah Jahan in memory of his wife, Mumtaz Mahal, in Agra, India. Here are some interesting facts and facts about the Taj Mahal:\n",
            "\n",
            "**History:**\n",
            "\n",
            "* The Taj Mahal was built between 1632 and 1653 by Shah Jahan, the Mughal Emperor, in memory of his wife, Mumtaz Mahal, who died in childbirth in 1631.\n",
            "* The construction of the Taj Mahal took 22 years to complete, from 1632 to 1653.\n",
            "* The mausoleum was designed by architects Ustad Ahmad Lahori and Ustad Ismail, and took 28 years to design and build.\n",
            "* The Taj Mahal was built using white marble from the Makrana in Rajasthan and red sandstone from Makrana, Rajasthan.\n",
            "\n",
            "**Architecture:**\n",
            "\n",
            "* The Taj Mahal is a large mausole\n",
            "\n",
            " Conversation Tree State---\n",
            "> branch_0: Contains 2 turns.\n",
            "\n",
            " Your Prompt: where is nagaland?\n",
            "> Running Contextual Scaffolding Analysis...\n",
            "> CSA -> Entity deficit check passed. Score: 0.8\n",
            "> Decision: Same branch (Score: 0.80 > 0.65)\n",
            "> Continuing on same branch: branch_0\n",
            "\n",
            "> Generating response for: 'where is nagaland?...'\n",
            "> Using past_kv_cache: True\n",
            "> ğŸš€ KV Cache Optimization Active!\n",
            "> Reusing 400 cached tokens\n",
            "> New tokens: 16\n",
            "> Using manual generation to preserve KV cache optimization\n",
            "> Generated 50 tokens...\n",
            "> Generated 100 tokens...\n",
            "> Generated 150 tokens...\n",
            "> âœ… Manual KV cache generation successful!\n",
            "> Generated 200 new tokens\n",
            "> Total context preserved: 615 tokens\n",
            "\n",
            "> LLM: Nagaland is a state in the northeastern part of India, located in the north-eastern region of the country. It is one of the 28 states of India and is bordered by:\n",
            "\n",
            "* Myanmar (Burma) to the west\n",
            "* Arunachal Pradesh to the north\n",
            "* Manipur to the east\n",
            "* Tripura to the southeast\n",
            "* Mizoram to the south\n",
            "* The Bay of Bengal to the southwest\n",
            "\n",
            "Nagaland is a diverse region with a unique cultural and geographical landscape, featuring:\n",
            "\n",
            "* Mountains: The state is home to the Khasi Hills, the Naga Hills, and the Eastern Himalayas.\n",
            "* Rivers: The Naga River, the Brahmaputra River, and the Siang River are some of the major rivers that flow through the state.\n",
            "* Wildlife: Nagaland is home to a diverse range of wildlife, including the Indian rhinoceros, the Bengal tiger, and the Asiatic lion.\n",
            "\n",
            "**Places to visit in Nag\n",
            "\n",
            " Conversation Tree State---\n",
            "> branch_0: Contains 3 turns.\n",
            "\n",
            " Your Prompt: where is Nagaland?\n",
            "> Running Contextual Scaffolding Analysis...\n",
            "> CSA -> Running semantic fallback check.\n",
            "> Topic Similarity Score: 0.7252\n",
            "> Prompt is self-contained. Applying penalty.\n",
            "> Final Score: 0.36262136697769165\n",
            "> Decision: New branch (Score: 0.36262136697769165 < 0.65)\n",
            "> Starting new branch: branch_1\n",
            "\n",
            "> Generating response for: 'where is Nagaland?...'\n",
            "> Using past_kv_cache: False\n",
            "> First turn - Input tokens: 41\n",
            "> âœ… KV Cache optimization successful!\n",
            "> Preserved cache for next turn: 16 layers\n",
            "\n",
            "> LLM: Nagaland is a state located in the northeastern part of India. It is situated in the North-Eastern Himalayan region, bordering the country's western borders with Myanmar (Burma) to the west and east, and the states of Arunachal Pradesh and Assam to the south.\n",
            "\n",
            " Conversation Tree State---\n",
            "> branch_0: Contains 3 turns.\n",
            "> branch_1: Contains 1 turns.\n",
            "\n",
            " Your Prompt: tell me more about taj mahal\n",
            "> Running Contextual Scaffolding Analysis...\n",
            "> CSA -> Running semantic fallback check.\n",
            "> Topic Similarity Score: 0.2334\n",
            "> Prompt is not self-contained. No penalty applied.\n",
            "> Final Score: 0.23339606821537018\n",
            "> Decision: New branch (Score: 0.23339606821537018 < 0.65)\n",
            "> Starting new branch: branch_2\n",
            "\n",
            "> Generating response for: 'tell me more about taj mahal...'\n",
            "> Using past_kv_cache: False\n",
            "> First turn - Input tokens: 43\n",
            "> âœ… KV Cache optimization successful!\n",
            "> Preserved cache for next turn: 16 layers\n",
            "\n",
            "> LLM: The Taj Mahal! One of the Seven Wonders of the World and a symbol of love and beauty. Here's more about this incredible monument:\n",
            "\n",
            "**History**\n",
            "\n",
            "The Taj Mahal was built in the 17th century, during the reign of Mughal Emperor Shah Jahan (r. 1627-1658). It was commissioned by his wife, Mumtaz Mahal, who was a queen of the Mughal Empire. The construction of the Taj Mahal took around 22 years to complete and began in 1632. The project was undertaken by skilled architects, artisans, and laborers, who were paid enormous sums of money for their work.\n",
            "\n",
            "**Design and Architecture**\n",
            "\n",
            "The Taj Mahal is a white marble mausoleum built in the shape of a dream. It is a perfect example of Mughal architecture, which blended Indian, Persian, and Islamic styles. The monument is surrounded by a beautiful garden, reflecting pools, and a series of smaller\n",
            "\n",
            " Conversation Tree State---\n",
            "> branch_0: Contains 3 turns.\n",
            "> branch_1: Contains 1 turns.\n",
            "> branch_2: Contains 1 turns.\n",
            "\n",
            " Your Prompt: exit\n"
          ]
        }
      ],
      "source": [
        "run_context_manager()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0pRBcteSlgcr"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
