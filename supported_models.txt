# Llama 3.2 Models (Recommended) 
"meta-llama/Llama-3.2-1B-Instruct"     # Currently in code
"meta-llama/Llama-3.2-3B-Instruct"     # Higher quality, more GPU memory
"unsloth/Llama-3.2-1B-Instruct"        # No gating required
"unsloth/Llama-3.2-3B-Instruct"        # No gating required

# Qwen 2.5 Models (Excellent for T4)
"Qwen/Qwen2.5-0.5B-Instruct"          # Ultra lightweight
"Qwen/Qwen2.5-1.5B-Instruct"          # Great balance
"Qwen/Qwen2.5-3B-Instruct"            # Higher quality
"Qwen/Qwen2.5-7B-Instruct"            # Premium quality (may be tight on T4)

# Phi-3 Models
"microsoft/Phi-3-mini-4k-instruct"     # 3.8B params, excellent quality
"microsoft/Phi-3-mini-128k-instruct"   # Long context version
"microsoft/Phi-3-small-8k-instruct"    # 7B params, needs more memory

# Gemma 3 Models
"google/gemma-3-1b-it"                 # 1B params, efficient
"google/gemma-2-3b-it"                 # High quality

# Works will with 15GB VRAM
RECOMMENDED_MODELS = {
    "best_overall": "Qwen/Qwen2.5-1.5B-Instruct",
    "fastest": "Qwen/Qwen2.5-0.5B-Instruct", 
    "highest_quality": "google/gemma-3-1b-it",
    "currently_working": "meta-llama/Llama-3.2-1B-Instruct"
}